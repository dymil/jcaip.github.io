---
layout: post
tags: [machine-learning, theory]
published: True
---

I'm going to be doing a series of blog posts about generative models, and how one can use them to tackle semi-supervised learning problems. In this post I'm going to give a general overview of several different generative models.


Generative models can be thought of a form of **unusupervised learning** or density estimation. 
We are trying to model the underlying distribution from the data. 

They can be used for super-resolution, colorization, and simulation.

Explcit density estimation - explicitly define and solve for $$p_{model}(x)$$

Implicit density estimation - learn a model to produce models from $$p_{model}(x)$$ without explicityl defining it. 

## PixelRNN/CNN
This is a tractable explicit model that is a type of fully visible belief network.

$$ p(x) = \prod_{i=1}^n p(x_i| x_1 \ldots x_{i-1}) $$

We train this using MLE, maxamizing the likelihood of the training data. Use neural netowrk to express the distribution.

Will need to define the ordering of pixels - starting from the corner. Each dependency is modeled via a LSTM RNN.
However, this sequential generation is really slow. 

PixelCNN - uses the same approach as PixelRNN, but the dependency is now codified by a CNN over a specified context region. 

Train using MLE - output is a softmax loss at each pixel. Max the likelihood that our training data is generated.

Training is faster than PixelRNN, we can also parallelize convolutions. However, generation is still slow, as it is sequential.

Chain rule basically defines a tractable density that is easy to work with. Gives a good evaluation metric of our data via the training likelihood. Unfortunately, it's stil sequential generation.

## Variational Autoencoders

Unlike PixelCNN/RNN, we define an intractible density function

$$p_\theta(x) = \int p_\theta(z) p_\theta(x \vert z) dz$$

$$z$$ is a **latent variable** from which our trainig data is generated from.
We can't optimize this directly, so instead we derive and optimize a lower bound on the likelihood instead.

### Autoencoders
Unsupervised approach to learning a lower-dimensional feature representation.
- Encoder network to map some feature vector $$z$$.
- Decoder maps $$z$$ back to the original input. 

We can train by minimizing L2 loss between the input and the output.

Encoder can be used to initialize a supevised model as a feature map. 

We assume our training data is generated from an unobserved, latent representation $$z$$.


### Variational Autoencoders
Sample from a prior over $$p_\theta(z)$$ and then generate our data, $$x$$ by sampling from our conditional distribution $$p_\theta(x \vert z)$$.

We want to estimate the true parameters of our model, $$\theta^*$$. 

Our prior should be a simple - unit Gaussian.
Our conditional distribution can be represented this with a neural network - this is the decoder network.

We can try to train using MLE, but $$p_\theta(x) = \int p_\theta(z) p_\theta(x \vert z) dz$$ is intractible. 

$$p_\theta(z)$$ is a simple gaussian, which is fine. 
$$p_\theta(x \vert z)$$ is a simple gaussian, which is fine. This is just the output of our neural network.

However, the integral makes this expression intractable - it's impossible to compute $$p(x \vert z)$$ for every z. 

So we can try to train using MAP instead of MLE, but we see that our posterior desnsity, 

$$ p_\theta(z \vert x) = \frac{ p_\theta(x \vert z) p_\theta(z) }{p_\theta(x)} $$

is also intractable, as we cannot calculate $$p_\theta(x)$$

The solution is to add a encoder network, $$q_\phi(z \vert x)$$ which allows us to derive a lower bound.
This encoder networks models our posterior probability - $$ p_\theta(z \vert x) $$

Because we are modeling probablistic generation, we generate a vector of **means** and **covariance** that represents $$z$$.

In order to get $$z$$, we will sample from the distribution that is generated by these two vectors.

### Deriving our data likelihood

We're deriving the log lokielihood here.

$$\log p_\theta(x) = E_{z \sim q_\phi(z \vert x)} \big[ \log p_\theta(x) \big]$$ 

We're taking the expectation with respect to $$z$$, as $$p_\theta(x)$$ does not depends on $$z$$

We can rewrite the right hand side of this equation using Bayes rule

$$ = E_z \big[ \log \frac{ p_\theta (x \vert z) p_\theta(z) }{ p_\theta(z | x)} \big]$$

We can multiply by a constant to get 

$$ = E_z \big[ \log \frac{ p_\theta (x \vert z) p_\theta(z) }{ p_\theta(z | x)} \big]$$

