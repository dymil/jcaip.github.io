---
layout: post
tags: [systems, theory]
published: True
---

When we write programs for computers, we often abstract away memory as a large, fast contiguous byte-addressable array. 
But we have a problem when we try to implement this abstraction in hardware - it turns out that it's impossible to satisfy all these conditions. 

It's possible to make a small amount of fast memory, and it's also possible to make a large amount of slow memory. 

By building the memory heirarchy, namely a series of caches, we can give the appearance that our memory is both fast and large. 

## Caching
Caching exploits **temporal locality** as well as **spatial locality**. Basically, the distribution of memory accesses is not uniform - if we access a specific block of memory, it's very likely that we will repeatedly access that block. It's also very likely that we will access the memory spots right next to that spot.


A memory address that is entered into the cahce is split into the tag, the index, and the block offset.
When we lookup in the cache, we take our address and use the tag to query the cache. 



![cache](http://sit.iitkgp.ernet.in/~coavl/images/dmc5.png)

think of


### Virtual Memory

Programs assume that they have access to all the memory available to the system. To them, memory looks like an infinitely large contiguous byte-addressable array. 

The OS is responsible for providing every program running on the computer with this abstraction. To do so it uses a page table to turn virtual addresses into physical ones. 

The MMU is responsible for taking in virtual addresses and outputing physical addresses. It maintains a cache called the TLB, or translation lookaside buffer. 

Basically every virtual address and translation is stored inside a page table. In early virtual memory systems, the TLB didn't exist and thus each translation required an access to the page table held in main memory - this is problematic because it breaks down the memory heirarchy that we set up earlier - the page table is too big to fit in the cache and thus resides in main memory, and since every access to memory has to go through this page table, we're bottlenecked by out translation speed. 

The TLB is a cache of common memory translations. 

In the worst case, when the page table may not be in memory. In this case, we must pull the page table from the disk, invoking an even larger time penalty.

### Caching and Virtual Memory
Here we have a problem - we can either cache on the **virtual address** or on the **physical address**. Each of these have different benefits

1. Physically indexed, physically tagged (PIPT)
    - In this kind of cache, the physical address is used for both the index and the tag. This is great, because we have no problems with coherency or aliasing. 
    - On the other hand, this now means we need to do a TLB lookup **every** time we access memory. Furthermore, we may pay an even greater penalty if our memory translation is not in the TLB - we must then access main memory, which takes even more time

2. Virtually indexed, virtually tagged (VIVT)
    - Uses the virtual address for both the index and the tag
    - Cache lookup can happen in parallel with TLB lookup
    - However, there are problems where Process A uses a virtual address, and then Process B uses the same virtual address. In this situation, our cache will give the wrong value for the lookup.
    - The same virtual address mapping to different physical addresses is also difficult to deal with in this setup.

3. Virtually indexed, physically tagged (VIPT)
    - This enables the MMU TLB lookup to proceed in parallel with fetching the data from the cache RAM, just like VIVT caches.
        - Must wait until physical address is given by MMU before finishing.
    - However, we can address homonyms and don't deal with the same aliasing problems as VIVT caches.
    - Regardless, we can still have the synonym problem, where two physical RAM blocks are in the cache, occupying two slots but with differetnt virtual indexes.

In general if we can do address translation via the TLB before the cache lookup finishes, then PIPT works best.
