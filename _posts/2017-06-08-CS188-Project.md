---
layout: post
title: CS188 Project
published: False

images:
    - url: /images/cs188/rcnn_structure.png
      alt: structure
      title: structure
---

## Literature Review
A literature review of some of the tools we used can be found [here](/CS188-Literature-Review/)

## Process/Overview
Our goal was to try and mimic the approach used in , but with MRI data instead of Chest X-Ray data.

To do this we first tried to categorize the data given to us into categories to train a CNN on these labels.


Unfortunately, we weren't able to get the corresponding images to our MRI documents, which stopped our efforts

Instead we shifted our focus entirely onto the textual data. We aimed to be able to provide information that doctors may have missed when writing the report.

## Handling the Data
To do so, we created a test set of 50 documents, and removed several key medical phrases from them by using the gensim summarizer. Because the data is not completely annonimized, we cannot share it, but here are a couple of annonimized examples

We treated the rest of our set of documents as training data.

We use a script provided [here](parsing) that parsed the documents and inserted them in to a MongoDB instance, which made querying and loading documents later on much easier.

The idea of this was to split on regular expressions and insert the full text into the database. Later, when we try to train on the data, we will parse and clean it.

## Training paragrpah vectors
Using Gensim, we implemented paragraph vectors for each document. 

As a sanity check, we compared to see how many paragraphs in the training set are most simlilar to themselves.

bar graph of similarity by counts

#### Preprocessing Data
First, we only pulled a couple of pertinent fields from our database of reports for each document. 
Next we lowercased and tokenized the string to be able to provide a corpus for gensim to train on.

## Topical Analysis
We used LDA to try and create topics for the data. We eventually settled upon 8 topics.

We improved on this by using HDP-LDA to get a right number of topics

We show a sample topic here

Several key things stick out, namely restricted diffusion, etc.

To try and visualize our topics, we trained word vectors on the data and normalized the lda topics, then plotted them on our graph

## Finding missed phrases
to do this, we created a phrase extractor (Jorge).

We specified a similarity threshold for both the paragrpah and lda vectors in order, then found all documents within that similarity threshold.

Next we ran our phrase extractor on our document as well as all documents within this threshold.

We displayed the most common missed phrases.

## Tuning our distance threshold
We then tried to optimize our distance thresholds via simulate annealing.
(simulated annealing graph)

## Performance on test set

## Code

## Conclusion

Why 
