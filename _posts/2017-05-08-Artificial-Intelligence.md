---
layout: post
title: Artificial Intelligence 

images:
    - url: https://www.deepcoredata.com/wp-content/uploads/2016/06/small_1420.png
      alt: Artificial Intelligence
      title: ida*
---

## Determining Intelligence

#### Turing Test
The Turing Test is a test devised by Alan Turning. The test calls for a computer and a human to be put in two seprate rooms, and another human to converse with both the human and the computer. If the human observer thinks he/she is talking to another human, the computer is said to have passed the Turing test. 

#### Winograd Schema
Winograd Schema are another series of problems that are used to test for understanding.
The idea is to have a sentence, with two possible meanings and to let the computer pick two choices that make sense. 

E.g. The city councilmen refused the demonstrators a permit because they [feared/advocated] violence. Who [feared/advocated] violence?


## Search
Many problems, such as 8-puzzles, n-queens, or solving a rubic's cube can be formulated as search problems.

### Formulating Search Problems
All search problems should have these parts:
+ **initial state** - This is the initial state that is given to the search problem
+ **goal state** - There may be more than one goal state. It is the state at which the search problem is solved.
+ **transition model/successor function** - Describes how to move from one state to another given an action. 
+ **action** - Each action changes the current state. Each action may also have an associated cost with it. 

The goal is to find a sequence of actions to move from the initial state to the goal state and minimize the cost of your actions

### Solving Search Problems
The idea behind solving all search problems is the same.
``` python
frontier = {I}
loop:
    if frontier is empty:
        return FALSE
    node = some node in the frontier
    if goal == node:
        return current state
    generate the node's children
    add the node's children to the frontier and "explored set"
```
A search strategy is said to be **complete** if it finds a solution if one exsists.

A search strategy is said to be **optimal** if the solution it finds is the one with the least cost.

The **seperation property** states that if the frontier seperates all explored states from all non-explored states that the search strategy is **optimal**

The **branching factor** describes how many actions you can take in a given state. If this number is not constant, the average branching factor or the max branching factor is usually used.

#### Breath First Search
The idea of BFS is to search the closest nodes first. To implement it, we use a queue (FIFO) to represent the frontier. 
However the problem with BFS is that it is exponential in time and space. Exponential time is not as large of a problem as exponential space, as we quickly run out of memory.
We can see that in order to expand all the nodes down to our solution depth, $$d$$, we would have to expand $$b^d$$ nodes, where $$b$$ is the branching factor. 

Let $$N(b, d)$$ be the number of nodes generated for a problem with branching factor $$b$$ and solution at depth $$d$$.

$$N(b, d) = b^d + b^{d-1} + \ldots + 1$$

$$b \times  N(b, d) = b^{d+1} + b^{d-1} + \ldots + b$$

$$b \times  N(b, d)  - N(b,d)= b^{d+1} - 1$$

$$N(b, d) = \frac{b^{d+1} + 1}{b-1} \approx \frac{b^{d+1}}{b-1} \approx b^d \times \frac{b}{b-1}$$

#### DFS
In DFS, we try to go as deep as possible first. To implement this, we use a stack (FILO) to represent the frontier.
We can see that our "explored set" needs only to be $$bd$$ rather than $$b^d$$. However, DFS is not optimal - our solution may not be the best one.

For both BFS and DFS, the number of nodes generated is 

$$N(b, d) \approx b^d \times \frac{b}{b-1}$$

#### Iterative Deepening Search
The idea of Iterative Deepening Search is to do Depth Limited Serach (DFS with a depth limit), gradually increasing the depth each time. This maintains the ideal properties of $$bd$$ space. While it may seem wasteful to regenerate each node, it actually only requires twice the work, maintaining an $$b^d$$ runtime as well as optimality.


Let $$N(b, d)$$ be the number of nodes generated by DFS for a problem with branching factor $$b$$ and solution at depth $$d$$.
We know from above that

$$N(b, d) \approx b^d(\frac{b}{b-1})$$

Let $$N'(b, d)$$ be the number of nodes generated by Iterative Deepening for a problem with branching factor $$b$$ and solution at depth $$d$$. 
Since iterative deepening runs for each depth from $$1$$ until $$d$$, we can express the number of nodes generated as a sum.

$$N'(b, d) = \sum_{i = 1}^{d}{b^i(\frac{b}{b-1})}$$

$$N'(b, d) = (\frac{b}{b-1})\sum_{i = 1}^{d}{b^i}$$

Now we can see that $$\sum_{i = 1}^{d}{b^i}$$ is the exact same equation we had for DFS before, so we et

$$N'(b, d) = (\frac{b}{b-1})^2b^d$$

$$\frac{b}{b-1}$$ is the difference between running DFS and ID.
Since $$b > 1$$, at most it is a 2x more work. We can see that even for relatively small $$b$$ ($$b=10$$) that it is just 1.1x more work.

#### Comparison

|        |BFS       |DFS       |ID        |
|--------|----------|----------|----------|
|Time    |$$O(b^d)$$|$$O(b^d)$$|$$O(b^d)$$|
|Space   |$$O(b^d)$$|$$O(bd)$$|$$O(bd)$$|
|Optimal | Y        |  N       |   Y      |
|Complete | Y        |  Y       |   Y      |

### Solving Search Problems with Cost

#### Uniform Cost Search
Nodes are expanded in increasing order of cost. Therefore, whatever we expand will have a larger cost.

#### Best First Search
An estimate of the cost at the current state is needed $$h(x)$$, or a heurestic function.
We can then use a greedy search - try to take the biggest bite at the current distance.
However, while this search strategy is complete it is not optimal.

#### Determining a Heurestic
Our heurestic is **consistent** if $$h(n) \leq c(n, a, n') + h(n')$$ where $$h(n)$$ is the cost for a node $$n$$, $$c(n, a, n')$$ is the cost to the goal state $$a$$ through $$n'$$. This means that the estimate is always less than or equal to the estimated distance from any neighboring vertex to the goal, plus the cost of reaching that neighbor.

Our heurestic is **admissable** if it never overpredicts the distance to the goal state. This means that $$h(n) \leq h^*(n)$$, where $$h^*(n)$$ is the actual cost to reach the goal state.

One easy way to generate heurestics is to relax the constraints of a problem. 

#### A\* Search
We can improve upon best first search by taking into account the cost used to get to a node as well as the predicted cost. We can model this with the function

$$f(n) = g(n) + h(n)$$

$$g(n)$$ = the cost to get to the current node

$$h(n)$$ = the predicted, heurestic cost to get to the final solution

Note that if $$h(n) = 0$$, then A\* is just Uniform Cost Search

A\* Search is optimal if our heurestic is both **consistent** and **admissable**.

For any node $$n$$, along the path from the initial state $$I$$ to a goal state $$G$$ with actual cost $$c^*$$, we know that $$f(n) = g(n) + h(n) \leq c*$$, since our heurestic is both consistent and admissable.

Suppose there is some other goal state, $$G'$$, that has less cost than $$c*$$.
$$f(G') = g(G') + h(G') = g(G') \leq c*$$. However, in this case, we would expand $$G'$$ before $$n$$, so we would reach the optimal solution.

### Local Search Strategies
+ Gradient Descent  
+ [Simulated Annealing](https://jcaip.github.io/Simulated-Annealing/)
+ [Genetic Algorithms](https://jcaip.github.io/Genetic-Algorithm/)

## Constraint Satisfaction
Constraint satisfication is similar to search problems. 

For a group of variables, $$x_1, x_2, \ldots , x_n$$, with domains $$D_1, D_2, \ldots , D_n$$, we want to find an assignment for each variable such that no constraints are violated.

From a problem, we can create a **constraint graph**, which is a visualization of the dependencies between variables. Each variables is represented by a vertex and an edge represents a constraint.

#### Types of Constraints
+ Unary Constraint - Constraints involving just one variable ($$A > 0$$).
+ Binary Constraint - Constraints involving two variables ( $$A \neq B$$).
+ Global Constraint - Constraints involving two or more variables ( $$A \neq B \neq C$$)

### Solving Contraint Satisfaction Problems

#### Backtracking Search
Naively, we can try to formulate a search problem, randomly assigning a variable a value from it's domain at each stage in the search tree. However, this means that the total # of states generated is $$n!d^n$$, where $$d$$ is the number of elements in th edomain and $$n$$ is the number of variables. 

We can exploit the fact that variable assignment is communatitive to speedup our serach. At each level, we assign only to a single variable that is determined. Using this, we get a runtime of $$O(b^d)$$, a significant improvement.

#### Heurestics
+ **Least Remaining Values** - when choosing a variable to assign, pick the variable that has the fewest states. 
+ **Degree Heurestic** - when choosing a variable to assign, pick the variable with the highest degree. The idea behind these two heurestics is to detect failure early, to avoid unnecessary expansion.
+ **Least Constraing Value** - when choosing a value to assign to a variable, pick the value that leaves the most remaining choices for the other unassigned variables.

#### Detecting Failure
+ **Local Consistency** - after each assignment, check that no constraints are violated
+ **Forward Checking** - after each assignment, look at all the neighbors and remove all remaining values. 
+ **Arc Consistency** - after each assignment, recursively apply forward checking, propogating information everytime you make an assignment

#### Exploiting Problem Structure
If our constraint graph is a tree, there is a $$P$$ solution to variable assignment. 
You may be able to use **Tree Decomposition** to break down your constraint graph into a tree, and then solve each tree with **message passing**.

## Games
Games can be broken down into different categories.

||**Deterministic**| **Chance**|
|--|--|--|
|**Perfect**|chess|backgammon|
|**Imperfect**  |battleship|poker|

### Formulating Games
In each game, there is said to be a min and max player, who take turns playing. A move by a min/max player is called a **ply**, while a **move** is when every player gets a ply.

Each game also has a **temrinal test** and a set of **operators**, much like search funcitons. 

Furthermore, every ame has a **utility function** that determines the value of the final state. The max player seeks to maxamize the utility function, while the min player seeks to minimize the function. 

If $$Utility(MAX) = -Utility(MIN)$$ the game is said to be a **zero-sum game**.

### Solving Games
#### Minimax
Minimax builds a search tree, with min and max nodes. A **min** node is the mininum of all its children, while a **max** node is the max of all its children. With this game tree, we are able to determine the optimal move by simply running DFS with backtracking to compute the min/max.

However, in practice, this tree is too large to build practically. Instead, we limit the depth of the tree and create a **Evaluation function**, which is an approximation of the current game state. 

We can use iterative deeepening to avoid keeping the whole tree in memory.

#### $$\alpha-\beta$$ Pruning
However, we can avoid looking at certain nodes with pruning. If we keep track of two values, $$\alpha$$, and $$\beta$$, we can prune roughly 1/2 of the nodes and avoid looking at them.

## Logic
We can use logic, along with a **Knowledge Base**, which consistents a set of sentences, in order to reason and draw conclusions. 

Logical reasoning is gaurenteed to be correct if the available information is correct.

**Duality of Logic** states that given any true statement/tautalogy we are able to derive another tautalogy by interchanging OR and AND and True and False.


To define logic, we have a set of variables, sentences and operations. And we try to capture the semantics of the real world through a logical representation in our logical syntax. 

If a sentence $$\alpha$$ is true in model world $$m$$, we say $$m$$ satisfies $$\alpha$$. $$M(\alpha)$$ is the set of models of $$\alpha$$

$$\alpha$$ **entails** $$\beta$$ if whenever $$\alpha$$ holds, $$\beta$$ holds as well. This is the same thing as saying $$M(\alpha) \subset M(\beta)$$.

We can check if a knowledge base entails something via **model checking**, simply enumerating all possible models to ensure $$M(KB) \subset M(\alpha)$$.

We can use this property for logical inference. An inference algorithm that derives only entailed sentences is **sound**, or **truth perserving**. It is **complete** if the algorithm can derive derive any sentence that is entailed. Finally we must consider **grounding** or converting our logical syntax into real world semantics.

### Propositional Logic
Propositional logic can be expressed as a set of operators, $$\neg, \land, \lor, \implies, \iff$$.

We say that two sentences are **logically equivalent** if they are true in the same set of models.

$$\alpha$$ is **valid** if it is true in every possible world.

$$\alpha$$ is **inconsistent** if it is not true in every possible world. If a model is not inconsistent, it is **satisfiable**. 

$$\alpha \models \beta$$ if and only if $$\alpha \land \neg \beta $$ is unsatisfiable.

$$\alpha$$ and $$\beta$$ are **mutually exclusive** if there are no worlds where both $$\alpha$$ and $$\beta$$ are true. 

#### Inference Rules and Resolution
**Modus Ponus** - $$\alpha \implies \beta, \alpha$$ infers $$\beta$$

**And-Elimination** - $$\alpha \land \beta$$ infers $$\beta$$ and $$\alpha$$

**montonicity** states that if $$KB \models \alpha$$, then $$KB \land \beta \models \alpha$$.

``` python
PL-Resolution(KB, a):
    clauses = KB $ !a
    for each pair of clauses, 
        resolvents = PL
        if resolvents contains the empty clause return true
        new =  new | resolvents
    if new is a subset of clauses return false
    clauses = clauses | new
```

Resolution is complete. To see this, we note define **resolution closure** to be the set of all clauses that our algorithm can derive. This set is finite, because there is a limited number of symbols to arrange. Hence our algorithm always terminates.

**ground resolution theorem** - If a set of clauses is unsatisfiable, then the resolution closure of those clauses contains the empty clause.

**defininte clauses** are a disjunction of literals where only one is positive. **Horn clauses** are a disjunction of literals where at most one is positive. Resolution on horn clauses returns another horn clause. HornClauseForm  = DefiniteClauseForm | GoalClauseForm

**Forward chaining** determines if a single proposition symbol, $$q$$ is entailed by the knowledge base. Tries to reason from a knowledge base to a query.
``` python
function forward_chaining(KB, q)
count where count[c] is the number of symbols in c's premise
inferred where inferred[s] is initially false for all symbols
agenda =  queue of known true symbols in the KB

while agenda is not empty
    p = agenda.pop()
    if p = q then return true
    if inferred[p] = false
        inferred[p] = true
        for each clause c in KB where p is in c.premise
            decrement count[s]
            if count[c] = 0 then add c.conclusion to agenda.
return false
```

**Backwards chaining** tries to reason backwards from $$q$$. 
If q is known, no work is needed. Otherwise, find all implications in the KB whose conclusio is q. If all the premises are true (proved via backward chaining) then q is true. This is an example of **goal-directed reasoning**.

#### Davis-Putnam-Logemann-Loveland
Algorithm for complete backtracking inference. Takes in a sentence in CNF form. 

**Early Termination** - algorithm detects whether a sentence must be true or false even with a partially completed model.

**Pure Symbol Heurestic** - a symbol is pure if it always appears with the same sign in all clauses, makes it easier to assign.

**Unit Caluse Heurestic** - a clause with just one literal or a clause in which all literals except one are assigned false. Cascades via **unit propogation**.

```python
DPLL-Satisfiable(s)
    clauses = set of clauses in the CNF representation of s
    symbols = set of propositional symbols in s
    return DPLL(clause, symbol, {})

DPLL(clauses, symbols, model)
    if every clause in clauses is true in model return True
    if some clause in clauses is false in model return False

    P, value = FindPureSymbol(symbols, clauses, model)
    if P return DPLL(clauses,symbols - P, model | P=value)

    P, value = FindUnitClause(clause, model)
    if P return DPLL(clauses,symbols - P, model | P=value)

    P = head(symbols), rest = rest(symbols)
    return DPLL(clauses, rest, model | P=true) or 
           DPLL(clauses, rest, model | P=false)
```

This can be used in conjunction with numerous tricks to speed up performance, namely **component analysis**, **variable and value ordering**, and **intelligent backtracking**, **random restarts**, and **clever indexing**.

We can also use local search to find a satisfiable equation. In general some SAT problems are **underconstrained**, that is they are likely to have a nearby solution. Emperically, we see that there is a threshold of clause/symbol at around 3. Beyond this number dramatically reduces the chance of a solution.

#### Converting to CNF 
1. Eliminate $$\iff$$ by replacing $$\alpha \iff \beta$$ with $$(\alpha \implies \beta) \land (\beta \implies \alpha)$$
2. Eliminate $$\implies$$ by replacing $$\alpha \implies \beta$$ with $$\neg \alpha \lor \beta$$
3. Remove $$\neg$$ outside of liters via double-negation elimination and DeMorgan's Law.
4. Distribute $$\lor$$ over $$\land$$ whenever possible.

## First-order Logic
First order logic exhibits **compositionality**, which means that the meaning of a sentnec is a function of the meaning of its parts.Predicate, or first order logic contains a predicate $$p(x)$$ that evaluates to T/F.

Models for first-order logic have objects in them. The **domain** of a model is the set of objects it contains. Objects may be related (represented by a set of tuples). Consists of **constant symbols**, **predicate symbols**, **function symbols**.

A **term** is a logical expression that refers to an object.

**Existential qualifier** - $$\exists x [p(x)]$$ there exists an $$x$$ such that $$p(x)$$ is true. Use $$\implies$$.

**Universal qualifier** - $$\forall x [p(x)]$$ $$p(x)$$ is true for all $$x$$. Use $$\land$$.

Quantifiers are related to each other in the following way: $$\neg(\forall x [p(x)]) = \exists x [\neg p(x)]$$

$$\neg(\exists x [p(x)]) = \forall x [\neg p(x)]$$

We either use a quantifier or instantiate the predicate to turn it to T/F.

If we make the **unique-names assumption** (every object has a distinct name), the **closed-world assumption** (atomic scentence not known to be true are false), and **domain closure** (each model contains no more domain element than those named by constant symbols) we recieve a database language. 

### First-Order Reasoning

#### Inference Rules
**Universal Instantiation** - subsitute a **ground term** for the variable.

**Existential Instantiation** - subsitute a **Skolem constant** for the variable.

This technique of **propositionalization** can be made completely general, since any sentence sentence entailed by the original knowledge base can be proved with a finite subset of the knowledge base. In general, the question of entailment for first order logic is **semidecidable**.

**Generalized Modus Ponens** - for atomix sentences $$p_i, p_i', q$$ where this a subsitution $$SUB(\theta, p_i') = SUB(\theta, p_i)$$ for all $$i$$.

$$\frac{p_i', (p_i \implies q)}{SUB(\theta, q)}$$


Lifted inference rules require finding subsitutions, called **unifications** to make different logical expressions look identitical. 

$$Unify(p, q) = \theta$$ where $$SUB(\theta, p) = SUB(\theta, q)$$ 

In this case, we need to standardize apart the two sentences being unified, which means renaming variables. In general, there is a single **most general unifier** that is unique up to renaming and subistuting variables. To check unifiers, we **occur check**,which ensures that no variable itself occurs inside the term. $$S(x)$$ can't unify with $$S(S(x))$$.

#### Resolution
1. Eliminate implications
2. Move $$\neg$$ inwards
3. Standardize variables - change variables so that the same variable is not used twice
4. Skolemize - remove any existential quantifiers via Skolemization constants/function
5. Drop universal quantifiers
6. Distribute $$\lor$$ over $$\land$$

Resolution inference is simply a lifted version of normal resolution. Two clauses can be resolved if one unifies with the negation of the other. This is **binary resolution**. We also need **factoring**, the removal of redundant ordering to get a complete solution.

However, resolution can give us **nonconstructive proofs**. We can combat this to add a special **answer literal** to the negated goal. 

Resolution in general is **refutation complete*** - that is if a a set is sentences is unsatisfiable, then resolution will be able to derive a contradiction. It cannot be used to generate all logical consequences of a set of sentences, but it can be used to establish a given sentence is entailed. 

1. If $$S$$ is unsatisfiable, then there exists a particular set of ground instances such that this set is also unsatisfiable
2. Propositional resolution is complete for ground instances
3. Any propositionl resolution proof for fround sentences can be formulated into a first order resolution proof.

Finite number of literals anyways, so finite runtime. **Godels Incompletness Theorem** states that there are true statements that we are unable to prove.

## Reasoning under Uncertainty
We handled uncertainty before via **belief states**. However, when interperting partial sensor information, belief states create a huge number of states. This leads to the **qualification** problem. 

In general, **laziness**, **theoretical ignorance** and **practical ignorance** keep us from deducing with pure logic. To deal with **degrees of belief** we will use **probability theory**. 

We use a combination of probability theory and utility theory to form deciscions, seeking to maximize the **max expected utility**. 

### Probability
The set of all possible worlds is called the **sample space**. $$P(w)$$ specifies the probability of each possible worlds. 

We use probabilities to create **propositions**. For any proposition $$\phi, P(\phi) = \sum_{\omega \in \phi}P(\omega)$$

Probabilities such as P(same number) are **unconditional** or **prior** probabilities. Usually however, we have some **evidence** and want to find the **conditional** or **posterior** probability of rolling doubles given the first die is a 5.

$$P(A\land B) =  P(A | B) P(B)$$

Variables in probability theory are refered to as **random variables** with a set domain. We use a **probability density function** to express the belief of continuous variables. 

For distributions on multiple variables, we use a **joint probability distribution**.  A full joint probability distribution is a probability model that is completely dependent on all of the random variables. 

**inclusion-exclusion principle** - $$P(a \lor b) = P(a) + P(b) - P(a \land B)$$

**de Finetti** - If an agent expresses a set of beliefs that violate probability theory, there is a combination of bets that gaurentees that that agent will lose money every time.

Probability has different schools of thought - frequentist, objectivist, subjectivist, subjective Bayesian view.

The **marginal** probability can be found by summing out, or **marginalization**. $$P(Y) = \sum_{z \in Z} P(Y,z)$$. If we subsitute in conditional probabilities using the product rule, this is called conditioning.

We can avoid a costly division by simply writing $$ P(X | e) = \alpha P(X, e) $$

**marginal independence** occurs when $$P(X | Y) = P(X)$$. If the complete set of variables can be divided into independent subsets, then the full joint distribution can be factored into seprate joint distributions.

**Bayes rule** - $$P(b | a) = \frac{P(a | b)P(b)}{P(a)}$$ We can use this when we see evidence of the effect of some unknown cause. Diagnostic knowledge is often more fragile than causal knowledge. 

If we assume **conditional independence**, then we can rerite $$P(A \land B | C) =  P(A | C) P(B | C)$$. This grows in size $$O(n)$$ instead of $$O(n^2)$$. It also introduces the concept of **seperation**.

We can then write the full joint probability distribution as

$$P(Cause, Effect_1 \ldots Effect_n) = P(Cause) \prod_i{P(Effect_i | Cause)}$$

This is the **naive-Bayes** model, which can be used as a **Bayesian classifier**.

## Bayesian Networks
We can use a **Bayesian network** to represent dependencies among variables. Each node corresponds to a random variable, which may be discrete or continuous. Each node $$X_i$$ has the probability distribution $$P(X_i | Parents(X_i))$$

Each node has a **conditional probability table**, with each row containing a **conditioning case**, which is just a possible combination of parent variables. 

We see that for each node, $$P(x_1 \ldots x_n) = \prod_{i=1}^n P(x_i \vert parents(X_i))$$

When we rewrite $$P(x_1 \ldots x_n)$$ into conditional probabilities, we see a **chain rule**. This means that our bayesian network is the correc representation if each node is conditionally independent from its predecessors given the parents.

Bayesian networks are a form of **locally-structured**, or **sparse** systems, where each subcomponent only interacts with a limited number of other nodes. If we choose the node ordering well, we are able to maintain a compact Bayesian network. if we try to build diagnostic models with links from symptoms to causes, we often have to specify additional dependencies vetween otherwise independent causes. 

Each variable is conditionally independent of its non-descendants given its parents. A node is conditional independent of all other nodes in the network given its children, parents, and children's parents, or its **Markov blanked**.

### Inference in Bayesian Networks
The basic task of probabilistic inference is to compute the posterior probaiblity for a set of **query variables**. That is we want $$P(X|e)$$ where $$e$$ is some particular observed event, and $$X = Hidden \cup  Evidence \cup Query$$.

We can conduct inference by enumeration by computing sums of products of conditional probabilities from the network.
    $$P(b|j,m) = \alpha \sum_e \sum_a P(b)P(e)P(a|b, e)P(j|a)P(m|a)$$

However, this can be set up via **variable elimination**. This works by evaluating expressions in right-to-left order and storing intermediate results. We split eahc part of the expression into corresponding **factors**.

This process will create new factors that eventually lead to our solution.

$$f(X_1 \ldots X_j, Y_1 \ldots Y_k, Z_1 \ldots Z_l) = f_1(X_1 \ldots X_j, Y_1 \ldots Y_k) f_5(Z_1 \ldots Z_l)$$

We can sum out factors $$f(B,C) = \sum_a f_3(A,B,C) = f_3(a, B,C) + f_3(\neg a, B,C)$$. Any factor that does not depend on the variable to be summed out can be moved outside the summation.

We need to be able to establish an ordering for the variables. Intractable to find optimal factor, but one good heurestic is to eliminate whichever variable minimizes the size of the next factor to be considered. 

It's important to note that every variable that is not an ancestor of a query variable or evidence variable is irrelevant to the query.

## Machine Learning
An agent is **learning** if it improves its performance on future tasks after making observations about the world.

+ What component should be improved?
+ What prior knowledge do we have?
+ What representation is used for the data and component?
+ What feedback is available to learn from?

Data will be provided in a **factored representation** - a vector of attribute values and outputs that can either be numerical or discrete.

**Feedback** can be classified into unsupervised, reinforcement, and supervised learning. There are also approachs that blur the line, for example semi-supervised learning or inverse reinforcement learning.

### Supervised Learning
In supervised learning, giving a **training set**, we try to find a **hypothesis** function $$h$$ that will give a proper matching. We evaluate $$h$$ on our **test set** of distinc examples.

When we want discrete values, this is **classification**, wheras when we want continuous values this is **regression**.

In general, we select $$h$$ from some **hypothesis space**. **Ockham's razor** suggests that we should pick the simplest hypothesis consistent with the data. In general, there is a tradeoff between complex hypotheses that fit the training data well and simpler hypotheses that may generalize better.

We say that a learning problem is **realizable** if the hypothesis space contains the true function. There is a tradeoff between the expressiveness of a hypothesis space and the complexity of finding a good hypothesis within that space.

### Learning Decision Trees
A decision tree represents a function that takes in a vector of attribute values and returns a single output value. The aim is to learn a definition for the **goal predicate**. A Boolean eciscion tree is good for some but not all applications. For example, a simple majority funciton is very hard to map. 

```python
DecisionTreeLearn(examples, attributes, parent_examples)
    if examples is empty then return Plurality-Value(parent_examples)
    else if all examples have the same classification then return classification
    else
        A = argmax(Importance(a, examples))
        tree = new decision tree with root test A
        for each value v of A
            exs = {e such that s.A = v}
            subreee = DecisionTreeLearn(exs, attributes -A, examples)
            add a branch to tree with label (A=v) and subtree subtree

        return tree
```

To pick the corrrect attribute, we introduce the notion of **entropy**. 

$$H(V) =  - \sum_kP(k) lg(P_k)$$ 

The information gain is the expected reduction in entropy.

Decision trees may overfit the data, so we may have to prune the tree. We can determine irrelevant nodes via a significance test. We can also use **early-stopping** to find the optimal stopping point.
