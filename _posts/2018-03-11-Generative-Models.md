---
layout: post
tags: [machine-learning, theory]
published: False
---
Generatives models can be used for super-resolution, colorization, simulation and planning.

They can be thought of a form of **unusupervised learning** or density estimation. 
We want to learn the underlying distribution fo the data. 

Explcit density estimation - explicitly define and solve for $$p_{model}(x)$$
Implicit density estimation - learn a model to produce models from $$p_{model}(x)$$ without explicityl defining it. 

## PixelRNN/CNN
This is a tractable explicit model. Type of fully visible belief network.

$$ p(x) = \prod_{i=1}^n p(x_i| x_1 \ldots x_{i-1}) $$

We train this using MLE, maxamizing the likelihood of the training data. Use neural netowrk to express the distribution.

Will need to define the ordering of pixels - starting from the corner. Each dependency is modeled via a LSTM RNN.
However, this sequential generation is really slow. 

PixelCNN - uses the same approach as PixelRNN, but the dependency is now codified by a CNN over a specified context region. 

Train using MLE - output is a softmax loss at each pixel. 

Training is faster than PixelRNN, we can also parallelize convolutions. However, generation is still sequential.

Chain rule basically defines a tractable density that is easy to work with. Gives a good evaluation metric of our data via the training likelihood. Unfortunately, it's stil sequential generation.

## Variational Autoencoders

Unlike PixelCNN/RNN, we define an intractible density function
$$p_\theta(x) = \int p_\theta(z) p_theta(x \vert z) dz$$

$$z$$ is a **latent variable** from which our trainig data is generated from.
We can't optimize this directly, so instead we derive and optimize a lower bound on the likelihood instead.

### Autoencoders
Unsupervised approach to learning a lower-dimensional feature representation. Encoder network to map some feature vector $$z$$. Decoder maps $$z$$ back to the original input. 

Encoder can be used to initialize a supevised model.

Sample from a prior over z, and then sample from our conditional distribution. 
We want to estimate the true parameters of our model. Chose our prior to be simple - just a gaussian. 

p(x|z) is complex, so we can model this with a neural network.
We can try to train using MLE, but this is intractible.
$$p_\theta(z)$$ is a simple gaussian, which is fine. 
$$p_\theta(x \vert z)$$ is a simple gaussian, which is fine. 
Impossible to compute $$p(x \vert z)$$ for every z. 

Posterior density is also intractable, as we cannot calculate $$p_\theta(x)$$

The solution is to add a encoder network, $$q_phi(z|x)$$ which allows us to derive a lower bound.

Because we are modeling probablistic generation, we generate a vector of **means** and **covariance**.

In order to get $$z$$, we will sample from the distribution that is generated by these two vectors.

$$\log p_theta(x)$$
